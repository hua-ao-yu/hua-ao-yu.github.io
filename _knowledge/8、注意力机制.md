---
title: "注意力机制"
collection: knowledge
type: "knowledge"
permalink: /knowledge/注意力机制
date: 2024-10-01
location: "China"
---

======

# 1、什么是注意力机制

<br> &emsp; “注意力机制”是上个世纪90年代，认知科学领域的学者发现的一个人类处理信息时采用的机制。深度学习中的注意力机制（Attention Mechanism）是一种模仿人类视觉和认知系统的方法，它允许神经网络在处理输入数据时集中注意力于相关的部分。通过引入注意力机制，神经网络能够自动地学习并选择性地关注输入中的重要信息，提高模型的性能和泛化能力。
<br> &emsp; 以下这张图可以较好地去理解注意力机制，其展示了人类在看到一幅图像时如何高效分配有限注意力资源的，其中红色区域表明视觉系统更加关注的目标，从图中可以看出：人们会把注意力更多的投入到人的脸部。文本的标题以及文章的首句等位置。
<br> <img src='/images/blogs/knowledges/8、注意力机制/注意力机制概念.webp' width="500" style="display: block; margin: 0 auto;">

# 2、自注意力机制

### 2.1、自注意力基础

<br> &emsp; 自注意力机制的基本思想是，<font color="#92d050">在处理序列数据时，每个元素都可以与序列中的其他元素建立关联，而不仅仅是依赖于相邻位置的元素。</font>它通过计算<font color="#ffff00">元素之间的相对重要性</font>来<font color="#ffff00">自适应地捕捉元素之间的长程依赖关系</font>。
<br> &emsp; 具体而言，对于序列中的每个元素，自注意力机制计算其与其他元素之间的相似度，并将这些相似度归一化为注意力权重。然后，通过将每个元素与对应的注意力权重进行加权求和，可以得到自注意力机制的输出。
<br> &emsp; self-attension的几个参数，其中Q和K的维度是一样的，V和他们不一样：
<br> <img src='/images/blogs/knowledges/8、注意力机制/注意力机制公式.png' width="500" style="display: block; margin: 0 auto;">
例子（文字描述版）：
<br> <img src='/images/blogs/knowledges/8、注意力机制/文字版例子.png' width="1000" style="display: block; margin: 0 auto;"> <br>
例子（图解描述版）：
<br> <img src='/images/blogs/knowledges/8、注意力机制/图解版例子.png' width="2500" style="display: block; margin: 0 auto;">
W1（Wq）、W2（Wk）、W3（Wv）都是一样的（共用），多头注意力机制才不一样。得到的每一个b，都是包含全局信息的。 <br>
计算步骤：
<br> &emsp; 1、经过Word2Vec等方法将字符转换为向量a；
<br> &emsp; 2、初始化Wq、Wk、Wv；
<br> &emsp; 3、将a分别乘以Wq、Wk、Wv得到q、k、v；
<br> &emsp; 4、使用上面的公式先计算q1 * ki.T，注意这里是使用q1分别乘以所有的ki，得到相应的αi；
<br> &emsp; 5、αi除以根号下dk后经过softmax层，得到一个值；
<br> &emsp; 6、将第5步得到的值分别乘以vi；
<br> &emsp; 7、将所有的vi加和，得到b1；
<br> &emsp; 8、重复对其他的ai。
个人的理解： <br>
&emsp; q代表该字符的一些特征，k用来表示该字符与其它字符之间的关系。 <br>
由于使用的W1、W2、W3都是一样的，所以self-attention是不包含位置信息的。
<br> <img src='/images/blogs/knowledges/8、注意力机制/无位置信息.png' width="500" style="display: block; margin: 0 auto;">

### 2.2、位置编码

为了解决这个问题，可以使用位置编码。
<br> <img src='/images/blogs/knowledges/8、注意力机制/位置编码.png' width="500" style="display: block; margin: 0 auto;">
<br> <img src='/images/blogs/knowledges/8、注意力机制/位置编码方法.png' width="500" style="display: block; margin: 0 auto;"> <br> 
如图所示，位置编码是加在输入的位置上。
这个可以理解成：
<br> &emsp; pos：第pos个词语，用到脑电领域可以理解为导联维度
<br> &emsp; i：每个词语可以转换成一个向量，这就是那个维度。脑电领域可以理解为时间维度
<br> <img src='/images/blogs/knowledges/8、注意力机制/位置编码公式.png' width="1000" style="display: block; margin: 0 auto;">

# 3、多头注意力机制

&emsp; 自注意力机制使用一组W1（Wq）、W2（Wk）、W3（Wv）多头注意力机制使用多组，其余的与自注意力机制相同。
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解.png' width="1000" style="display: block; margin: 0 auto;"> <br>
此外还有一种简化的方法：
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制（简化）.png' width="1000" style="display: block; margin: 0 auto;"> <br>
简化版的方法将q、k、v分别截取为h段，h为头的数量。每一个head的计算和self-attention相同。
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解详细1.png' width="1000" style="display: block; margin: 0 auto;">
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解详细2.png' width="1000" style="display: block; margin: 0 auto;">
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解详细3.png' width="1000" style="display: block; margin: 0 auto;">
最后将每一个head计算得到的bi拼接起来组成一个大的bi。<br>
计算过程示意图：
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制过程示意图.png' width="1500" style="display: block; margin: 0 auto;">

# 参考文献
[注意力机制综述（自注意力机制、多头、通道、空间）](https://zhuanlan.zhihu.com/p/631398525) <br> 
[多头注意力机制（很详细）](https://blog.csdn.net/zgpeace/article/details/126635650) <br> 
[各种深度学习中的注意力机制（部分有代码）](https://blog.csdn.net/amusi1994/article/details/118347925) <br> 
[自注意力机制（理论+代码）](https://www.bilibili.com/video/BV1qo4y1F7Ep/?spm_id_from=333.788.recommend_more_video.1&vd_source=d5bf9778119412295721f01394ea745e) <br> 
[使用3B1B的动态介绍（非常好）](https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.999.0.0&vd_source=d5bf9778119412295721f01394ea745e)
