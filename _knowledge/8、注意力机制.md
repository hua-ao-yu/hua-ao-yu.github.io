---
title: "注意力机制"
collection: knowledge
type: "knowledge"
permalink: /knowledge/注意力机制
date: 2024-10-01
location: "China"
---

自注意力机制的介绍和更多的变体。 / Self-Attention and its variants.
======

# 1、什么是注意力机制

<br> &emsp; “注意力机制”是上个世纪90年代，认知科学领域的学者发现的一个人类处理信息时采用的机制。深度学习中的注意力机制（Attention Mechanism）是一种模仿人类视觉和认知系统的方法，它允许神经网络在处理输入数据时集中注意力于相关的部分。通过引入注意力机制，神经网络能够自动地学习并选择性地关注输入中的重要信息，提高模型的性能和泛化能力。
<br> &emsp; 以下这张图可以较好地去理解注意力机制，其展示了人类在看到一幅图像时如何高效分配有限注意力资源的，其中红色区域表明视觉系统更加关注的目标，从图中可以看出：人们会把注意力更多的投入到人的脸部。文本的标题以及文章的首句等位置。
<br> <img src='/images/blogs/knowledges/8、注意力机制/注意力机制概念.webp' width="500" style="display: block; margin: 0 auto;">

# 2、自注意力机制

### 2.1、自注意力基础

<br> &emsp; 自注意力机制的基本思想是，<font color="#92d050">在处理序列数据时，每个元素都可以与序列中的其他元素建立关联，而不仅仅是依赖于相邻位置的元素。</font>它通过计算<font color="#ffff00">元素之间的相对重要性</font>来<font color="#ffff00">自适应地捕捉元素之间的长程依赖关系</font>。
<br> &emsp; 具体而言，对于序列中的每个元素，自注意力机制计算其与其他元素之间的相似度，并将这些相似度归一化为注意力权重。然后，通过将每个元素与对应的注意力权重进行加权求和，可以得到自注意力机制的输出。
<br> &emsp; self-attension的几个参数，其中Q和K的维度是一样的，V和他们不一样：
<br> <img src='/images/blogs/knowledges/8、注意力机制/注意力机制公式.png' width="500" style="display: block; margin: 0 auto;">
例子（文字描述版）：
<br> <img src='/images/blogs/knowledges/8、注意力机制/文字版例子.png' width="1000" style="display: block; margin: 0 auto;"> <br>
例子（图解描述版）：
<br> <img src='/images/blogs/knowledges/8、注意力机制/图解版例子.png' width="2500" style="display: block; margin: 0 auto;">
W1（Wq）、W2（Wk）、W3（Wv）都是一样的（共用），多头注意力机制才不一样。得到的每一个b，都是包含全局信息的。 <br>
计算步骤：
<br> &emsp; 1、经过Word2Vec等方法将字符转换为向量a；
<br> &emsp; 2、初始化Wq、Wk、Wv；
<br> &emsp; 3、将a分别乘以Wq、Wk、Wv得到q、k、v；
<br> &emsp; 4、使用上面的公式先计算q1 * ki.T，注意这里是使用q1分别乘以所有的ki，得到相应的αi；
<br> &emsp; 5、αi除以根号下dk后经过softmax层，得到一个值；
<br> &emsp; 6、将第5步得到的值分别乘以vi；
<br> &emsp; 7、将所有的vi加和，得到b1；
<br> &emsp; 8、重复对其他的ai。
个人的理解： <br>
&emsp; q代表该字符的一些特征，k用来表示该字符与其它字符之间的关系。 <br>
由于使用的W1、W2、W3都是一样的，所以self-attention是不包含位置信息的。
<br> <img src='/images/blogs/knowledges/8、注意力机制/无位置信息.png' width="500" style="display: block; margin: 0 auto;">

### 2.2、形象化解释（3B1B）
##### 2.2.1、transformer的作用
&emsp; transformer的目标是接受一段文本然后预测下一个词：
<br> <img src='/images/blogs/knowledges/8、注意力机制/transformer的目标.png' width="500" style="display: block; margin: 0 auto;">

##### 2.2.2、什么是token
&emsp; 每一个词或者字符或者单词片段被切成段，即为token：
<br> <img src='/images/blogs/knowledges/8、注意力机制/token的定义.png' width="500" style="display: block; margin: 0 auto;"> <br> 
每一个token的初始嵌入是一个高维向量，编码了该单词的含义和位置信息，和上下文没有关联。
<br> <img src='/images/blogs/knowledges/8、注意力机制/token编码单词和位置.png' width="500" style="display: block; margin: 0 auto;"> 

##### 2.2.3、嵌入向量及信息传递
&emsp; transformer的第一步将各个token关联到一个被称为<font color="#ff0000">嵌入向量</font>的高维模型。
<br> <img src='/images/blogs/knowledges/8、注意力机制/嵌入向量.png' width="500" style="display: block; margin: 0 auto;"> <br> 
在所有可能的嵌入向量构成的高维空间中，方向可以对应语义，如果是图片生成的话就是图片生成的内容。
<br> <img src='/images/blogs/knowledges/8、注意力机制/嵌入向量的方向对应语义.png' width="500" style="display: block; margin: 0 auto;"> <br> 
transformer的目标是逐步调整这些嵌入向量，使它们不单单编码单个词，还可以融入更丰富的上下文的含义。 <br>

例如，以下三句话中，在切片成token后每个“mole”的token的向量是一样的：
<br> <img src='/images/blogs/knowledges/8、注意力机制/例子mole1.png' width="500" style="display: block; margin: 0 auto;"> <br> 
当进入transformer之后，通过attention周围的信息才可以传递到该嵌入向量。
<br> <img src='/images/blogs/knowledges/8、注意力机制/例子mole2.png' width="500" style="display: block; margin: 0 auto;"> <br> 
而且attention模块不仅精细化了一个词的含义，还允许模型相互传递这些嵌入向量所蕴含的信息，甚至传递得很远，而且新嵌入向量的信息，可以比单个词丰富得多。 <br>
可以理解为，<font color="#ffc000">mole</font>再经过attention后，理解了上下文，变为了<font color="#e36c09">mole</font>、<font color="#00b0f0">mole</font>、<font color="#595959">mole</font>（理想情况）。
<br> <img src='/images/blogs/knowledges/8、注意力机制/例子mole3.png' width="500" style="display: block; margin: 0 auto;"> <br> 
而训练好的模型可以达到以下效果，即可以很好的指向性：
<br> <img src='/images/blogs/knowledges/8、注意力机制/例子mole4.png' width="500" style="display: block; margin: 0 auto;"> <br> 
最后的目标是通过一系列计算产生一组新的、更为精准的嵌入向量，比如那些经过形容词修饰的名词所对应的向量。
<br> <img src='/images/blogs/knowledges/8、注意力机制/例子蓝色生物.png' width="500" style="display: block; margin: 0 auto;"> <br> 

##### 2.2.4、查询（Query）

查询就是QKV中的Q。就是查询某个token（如creature）前包含信息的步骤，而这样的提问被编码成了另一个向量，即下图中的Query。
<br> <img src='/images/blogs/knowledges/8、注意力机制/查询.png' width="500" style="display: block; margin: 0 auto;"> <br> 
要计算查询向量，先要取得一个矩阵，即为WQ。这里需要把查询矩阵分别与文中的每个token（Ei）的嵌入向量相乘，得到Qi。Qi即为包含了前置信息的向量。
<br> <img src='/images/blogs/knowledges/8、注意力机制/Qi计算过程.png' width="500" style="display: block; margin: 0 auto;"> <br> 

##### 2.2.5、键（Key）

从概念上讲可以将“键”视为想要回答“查询”。 
<br> <img src='/images/blogs/knowledges/8、注意力机制/键的概念.png' width="500" style="display: block; margin: 0 auto;"> <br> 
当键与查询的方向对齐时，就能认为它们相匹配。
<br> <img src='/images/blogs/knowledges/8、注意力机制/键与查询相匹配.png' width="500" style="display: block; margin: 0 auto;"> <br> 
得到的结果类似这样，原点越大，点积越大，键与查询越对齐：
<br> <img src='/images/blogs/knowledges/8、注意力机制/键与查询相乘.png' width="500" style="display: block; margin: 0 auto;"> <br> 
如“蓝色毛茸茸生物”例子中，fluffy和blue与creature的键与creature所产生的查询高度对齐，而the与creature互不相关：
<br> <img src='/images/blogs/knowledges/8、注意力机制/蓝色毛茸茸与生物对齐.png' width="500" style="display: block; margin: 0 auto;"> <br> 
这一步就是Q x K，为了将数值限定到0-1并且所有值的和为1，使用softmax函数进行归一化，为了数值稳定性，建议将所有点积除以<font color="#c00000">键-查询空间维度的平方根</font>。

##### 2.2.6、值（Value）

使用值矩阵WV乘以嵌入向量，可以得到一系列值向量，可以想象成是与对应的键向量相关联。
<br> <img src='/images/blogs/knowledges/8、注意力机制/使用值矩阵WV乘以嵌入向量.png' width="500" style="display: block; margin: 0 auto;"> <br> 

再将值向量乘以前面计算出来的权重，注意，除了比较有价值的两个token（fluffy和blue），其余的词不是因为掩码而归零（creature下面的），就是在归零的路上（a）。
<br> <img src='/images/blogs/knowledges/8、注意力机制/值向量与权重相乘.png' width="500" style="display: block; margin: 0 auto;"> <br> 
然后将这一列求和作为更新量，得到更新后的嵌入向量。
<br> <img src='/images/blogs/knowledges/8、注意力机制/更新嵌入向量.png' width="500" style="display: block; margin: 0 auto;"> <br> 

##### 2.2.7、一种高效的训练方式（掩码）

让模型同时预测每个初始token子序列之后所有可能的下一个token，这样做的好处是一个训练样本可以提供多次训练机会。 <br> 
就注意力模式而言，这意味着不能让后面的词影响前面的词，不然会产生数据泄露。也就是说这些代表后方token影响前方的位置能被强制变为0，但是不能直接设置为0，不然每列的和就不是1了，所以要在softmax前设置为负无穷。
<br> <img src='/images/blogs/knowledges/8、注意力机制/下方设置为0.png' width="500" style="display: block; margin: 0 auto;"> <br> 
这一过程被称为“<font color="#c00000">掩码（masking）</font>”
<br> <img src='/images/blogs/knowledges/8、注意力机制/保持归一化.png' width="500" style="display: block; margin: 0 auto;"> <br> 

##### 2.2.8、自注意力机制的变体

注意力模式的大小等于上下文长度的平方（K和Q数量一致），这也是大语言模型的巨大瓶颈之一。 <br> 
为了解决这个问题，提出了一些注意力机制的变体（少了一个cross-attention）：
<br> <img src='/images/blogs/knowledges/8、注意力机制/注意力机制的变体.png' width="500" style="display: block; margin: 0 auto;"> <br> 


### 2.3、位置编码
（注：似乎有相关研究证明位置编码可能没用。）
为了解决这个问题，可以使用位置编码。
<br> <img src='/images/blogs/knowledges/8、注意力机制/位置编码.png' width="500" style="display: block; margin: 0 auto;">
<br> <img src='/images/blogs/knowledges/8、注意力机制/位置编码方法.png' width="500" style="display: block; margin: 0 auto;"> <br> 
如图所示，位置编码是加在输入的位置上。
这个可以理解成：
<br> &emsp; pos：第pos个词语，用到脑电领域可以理解为导联维度
<br> &emsp; i：每个词语可以转换成一个向量，这就是那个维度。脑电领域可以理解为时间维度
<br> <img src='/images/blogs/knowledges/8、注意力机制/位置编码公式.png' width="1000" style="display: block; margin: 0 auto;">

# 3、多头注意力机制

&emsp; 自注意力机制使用一组W1（Wq）、W2（Wk）、W3（Wv）多头注意力机制使用多组，其余的与自注意力机制相同。
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解.png' width="1000" style="display: block; margin: 0 auto;"> <br>
此外还有一种简化的方法：
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制（简化）.png' width="1000" style="display: block; margin: 0 auto;"> <br>
简化版的方法将q、k、v分别截取为h段，h为头的数量。每一个head的计算和self-attention相同。
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解详细1.png' width="1000" style="display: block; margin: 0 auto;">
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解详细2.png' width="1000" style="display: block; margin: 0 auto;">
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制图解详细3.png' width="1000" style="display: block; margin: 0 auto;">
最后将每一个head计算得到的bi拼接起来组成一个大的bi。<br>
计算过程示意图：
<br> <img src='/images/blogs/knowledges/8、注意力机制/多头注意力机制过程示意图.png' width="1500" style="display: block; margin: 0 auto;">

# 参考文献
[注意力机制综述（自注意力机制、多头、通道、空间）](https://zhuanlan.zhihu.com/p/631398525) <br> 
[多头注意力机制（很详细）](https://blog.csdn.net/zgpeace/article/details/126635650) <br> 
[各种深度学习中的注意力机制（部分有代码）](https://blog.csdn.net/amusi1994/article/details/118347925) <br> 
[自注意力机制（理论+代码）](https://www.bilibili.com/video/BV1qo4y1F7Ep/?spm_id_from=333.788.recommend_more_video.1&vd_source=d5bf9778119412295721f01394ea745e) <br> 
[使用3B1B的动态介绍（非常好）](https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.999.0.0&vd_source=d5bf9778119412295721f01394ea745e)
